{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88847a32-be8b-48ff-8c4e-77239871e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40664cb-de2c-4f9b-b54d-ff3ed741a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64341f00-6c29-4bc1-8a09-aa827d90d0bd",
   "metadata": {},
   "source": [
    "1. Data Acquisition : Web scraping our data from any website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262f8cde-64da-4176-b7b7-67dcb887fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = \"https://www.geeksforgeeks.org/python-web-scraping-tutorial/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd000bbc-beca-4003-a6f6-f361f71ccbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(my_url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f967af-e18b-4555-ab3a-0765755b0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "soupified = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9e52c-a0c1-467e-b098-a66a2615a67d",
   "metadata": {},
   "source": [
    "Finding all the paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0239a1-4c92-4daf-9ea2-d4019bda19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = soupified.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b10d09-9dba-4068-97ce-e5862945e124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p dir=\"ltr\"><span>Web scraping, the process of extracting data from websites, has emerged as a powerful technique to gather information from the vast expanse of the internet. In this tutorial, we’ll explore various Python libraries and modules commonly used for web scraping and delve into why Python 3 is the preferred choice for this task.</span></p>,\n",
       " <p dir=\"ltr\"><span>The latest version of </span><a href=\"https://www.geeksforgeeks.org/python-programming-language\" rel=\"noopener\" target=\"_blank\"><span>Python</span></a><span>, offers a rich set of tools and libraries specifically designed for web scraping, making it easier than ever to retrieve data from the web efficiently and effectively.</span></p>,\n",
       " <p style=\"margin:4px; font-size:20px; font-weight:bold;\">Table of Content</p>,\n",
       " <p dir=\"ltr\"><span>The requests library is used for making HTTP requests to a specific URL and returns the response. Python requests provide inbuilt functionalities for managing both the request and response.</span></p>,\n",
       " <p dir=\"ltr\"><span>Python requests module has several built-in methods to make HTTP requests to specified URI using GET, POST, PUT, PATCH, or HEAD requests. A HTTP request is meant to either retrieve data from a specified URI or to push data to a server. It works as a request-response protocol between a client and a server. Here we will be using the GET request. The </span><a href=\"https://www.geeksforgeeks.org/get-method-python-requests\" rel=\"noopener\"><span>GET method</span></a><span> is used to retrieve information from the given server using a given URI. The GET method sends the encoded user information appended to the page request. </span></p>,\n",
       " <p dir=\"ltr\"><span> </span><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><img alt=\"Python requests making GET request\" height=\"inherit\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20211023185655/PythonrequestsmakingGETrequest.png\" width=\"inherit\"/></p>,\n",
       " <p dir=\"ltr\"><span>For more information, refer to our </span><a href=\"https://www.geeksforgeeks.org/python-requests-tutorial\" rel=\"noopener\"><span>Python Requests Tutorial</span></a><span>. </span></p>,\n",
       " <p dir=\"ltr\"><span>Beautiful Soup provides a few simple methods and Pythonic phrases for guiding, searching, and changing a parse tree: a toolkit for studying a document and removing what you need. It doesn’t take much code to document an application.</span></p>,\n",
       " <p dir=\"ltr\"><span>Beautiful Soup automatically converts incoming records to Unicode and outgoing forms to UTF-8. You don’t have to think about encodings unless the document doesn’t define an encoding, and Beautiful Soup can’t catch one. Then you just have to choose the original encoding. Beautiful Soup sits on top of famous Python parsers like LXML and HTML, allowing you to try different parsing strategies or trade speed for flexibility.</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><img alt=\"Python BeautifulSoup Parsing HTML\" height=\"inherit\" loading=\"lazy\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20211023191537/PythonBeautifulSoupParsingHTML.png\" width=\"inherit\"/></p>,\n",
       " <p dir=\"ltr\"><span>Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. The website we want to scrape contains a lot of text so now let’s scrape all those content. First, let’s inspect the webpage we want to scrape. </span></p>,\n",
       " <p><br/></p>,\n",
       " <p dir=\"ltr\"><br/></p>,\n",
       " <p><br/></p>,\n",
       " <p dir=\"ltr\"><span>In the above image, we can see that all the content of the page is under the div with class entry-content. We will use the find class. This class will find the given tag with the given attribute. In our case, it will find all the div having class as entry-content. </span></p>,\n",
       " <p dir=\"ltr\"><span>We can see that the content of the page is under the &lt;p&gt; tag. Now we have to find all the p tags present in this class. We can use the </span><a href=\"https://www.geeksforgeeks.org/python-beautifulsoup-find-all-class\" rel=\"noopener\"><span>find_all</span></a><span> class of the BeautifulSoup.</span></p>,\n",
       " <p dir=\"ltr\"><span> </span><b><strong>Output:</strong></b></p>,\n",
       " <p dir=\"ltr\"><img alt=\"find_all bs4\" height=\"inherit\" loading=\"lazy\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20210323152640/findallbs4.png\" width=\"inherit\"/></p>,\n",
       " <p dir=\"ltr\"><span>For more information, refer to our </span><a href=\"https://www.geeksforgeeks.org/how-to-scrape-websites-with-beautifulsoup-and-python\" rel=\"noopener\" target=\"_blank\"><span>Python BeautifulSoup</span></a><span>. </span></p>,\n",
       " <p dir=\"ltr\"><span>Selenium is a popular Python module used for automating web browsers. It allows developers to control web browsers programmatically, enabling tasks such as web scraping, automated testing, and web application interaction. Selenium supports various web browsers, including Chrome, Firefox, Safari, and Edge, making it a versatile tool for browser automation.</span></p>,\n",
       " <p dir=\"ltr\"><span>In this specific example, we’re directing the browser to the Google search page with the query parameter “geeksforgeeks”. The browser will load this page, and we can then proceed to interact with it programmatically using Selenium. This interaction could involve tasks like extracting search results, clicking on links, or scraping specific content from the page.</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><img alt=\"for-firefox\" height=\"367\" loading=\"lazy\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20240308160217/for-firefox.png\" srcset=\"https://media.geeksforgeeks.org/wp-content/uploads/20240308160217/for-firefox.png 751w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160217/for-firefox-100.png 100w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160217/for-firefox-200.png 200w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160217/for-firefox-300.png 300w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160217/for-firefox-660.png 660w\" width=\"751\"/></p>,\n",
       " <p dir=\"ltr\"><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><img height=\"inherit\" loading=\"lazy\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20210226111840/scrapemultiplepagespythonselenium.png\" width=\"inherit\"/></p>,\n",
       " <p dir=\"ltr\"><span>For more information, refer to our </span><a href=\"https://www.geeksforgeeks.org/selenium-python-tutorial\" rel=\"noopener\" target=\"_blank\"><span>Python Selenium</span></a><span>. </span></p>,\n",
       " <p dir=\"ltr\"><span>The lxml module in Python is a powerful library for processing XML and HTML documents. It provides a high-performance XML and HTML parsing capabilities along with a simple and Pythonic API. lxml is widely used in Python web scraping due to its speed, flexibility, and ease of use.</span></p>,\n",
       " <p dir=\"ltr\"><span>Here’s a simple example demonstrating how to use the lxml module for Python web scraping:</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><span>The urllib module in Python is a built-in library that provides functions for working with URLs. It allows you to interact with web pages by fetching URLs (Uniform Resource Locators), opening and reading data from them, and performing other URL-related tasks like encoding and parsing. Urllib is a package that collects several modules for working with URLs, such as:</span></p>,\n",
       " <p dir=\"ltr\"><span>If urllib is not present in your environment, execute the below code to install it.</span></p>,\n",
       " <p dir=\"ltr\"><span>Here’s a simple example demonstrating how to use the urllib module to fetch the content of a web page:</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><img alt=\"uutt\" height=\"655\" loading=\"lazy\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20240308160419/uutt.png\" srcset=\"https://media.geeksforgeeks.org/wp-content/uploads/20240308160419/uutt.png 893w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160419/uutt-100.png 100w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160419/uutt-200.png 200w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160419/uutt-300.png 300w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160419/uutt-660.png 660w,https://media.geeksforgeeks.org/wp-content/uploads/20240308160419/uutt-768.png 768w\" width=\"893\"/></p>,\n",
       " <p dir=\"ltr\"><span>The pyautogui module in Python is a cross-platform GUI automation library that enables developers to control the mouse and keyboard to automate tasks. While it’s not specifically designed for web scraping, it can be used in conjunction with other web scraping libraries like Selenium to interact with web pages that require user input or simulate human actions.</span></p>,\n",
       " <p dir=\"ltr\"><span>In this example, pyautogui is used to perform scrolling and take a screenshot of the search results page obtained by typing a query into the search input field and clicking the search button using Selenium.</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><img height=\"inherit\" loading=\"lazy\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20210320104649/Mouseclick.gif\" width=\"inherit\"/></p>,\n",
       " <p dir=\"ltr\"><span>The schedule module in Python is a simple library that allows you to schedule Python functions to run at specified intervals. It’s particularly useful in web scraping in Python when you need to regularly scrape data from a website at predefined intervals, such as hourly, daily, or weekly.</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Output</strong></b></p>,\n",
       " <p dir=\"ltr\"><img height=\"inherit\" loading=\"lazy\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20210629200554/Screenshot20210629at80230PM.png\" width=\"inherit\"/></p>,\n",
       " <p dir=\"ltr\" style=\"text-align: start;\"><span>Python’s popularity for web scraping stems from several factors:</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Ease of Use</strong></b><span>: Python’s clean and readable syntax makes it easy to understand and write code, even for beginners. This simplicity accelerates the development process and reduces the learning curve for web scraping tasks.</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Rich Ecosystem</strong></b><span>: Python boasts a vast ecosystem of libraries and frameworks tailored for web scraping. Libraries like BeautifulSoup, Scrapy, and Requests simplify the process of parsing HTML, making data extraction a breeze.</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Versatility</strong></b><span>: Python is a versatile language that can be used for a wide range of tasks beyond web scraping. Its flexibility allows developers to integrate web scraping seamlessly into larger projects, such as data analysis, machine learning, or web development.</span></p>,\n",
       " <p dir=\"ltr\"><b><strong>Community Support</strong></b><span>: Python has a large and active community of developers who contribute to its libraries and provide support through forums, tutorials, and documentation. This wealth of resources ensures that developers have access to assistance and guidance when tackling web scraping challenges.</span></p>,\n",
       " <p dir=\"ltr\"><span>this tutorial has shown you the basics of how to use Python for web scraping. With the tools we’ve discussed, you can start collecting data from the internet quickly and easily. Whether you need this data for a project, research, or just for fun, Python makes it possible. Remember to always scrape data responsibly and follow the rules set by websites. If you’re excited to learn more about Python and web scraping, check out our </span><a href=\"https://www.geeksforgeeks.org/courses/python-course-certification-free\" rel=\"noopener\" target=\"_blank\"><span>Free Python Course</span></a><span>. It’s a great resource to deepen your understanding and enhance your skills, all while having fun exploring the power of Python.</span></p>,\n",
       " <p><span id=\"suggestion-modal-alert\" style=\"display: none;\"></span></p>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930735b6-bff1-4acc-b80e-10cab7b8b955",
   "metadata": {},
   "source": [
    "Finding the first paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "561bf11f-4f84-4281-865d-132e62a4c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_para = soupified.find_all('p')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b580c5df-5117-4597-89cc-9ae7b4a10077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web scraping, the process of extracting data from websites, has emerged as a powerful technique to gather information from the vast expanse of the internet. In this tutorial, we’ll explore various Python libraries and modules commonly used for web scraping and delve into why Python 3 is the preferred choice for this task.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d40b3-a722-485c-abde-988a3a100636",
   "metadata": {},
   "source": [
    "Finding the second paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be270f7d-bd72-4f69-a876-ffa8ddf8e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_para = soupified.find_all('p')[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78384f4f-90e8-4fd1-a09b-05ebc78e4e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The latest version of Python, offers a rich set of tools and libraries specifically designed for web scraping, making it easier than ever to retrieve data from the web efficiently and effectively.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89526d-4f40-4709-a98b-439118d95038",
   "metadata": {},
   "source": [
    "2. Clean Data : Data cleaning most often refers to cleaning the web-scraped data using Regex, and then correcting it's spelling, lemmatizing it, etc.\n",
    "\n",
    "But I am not going to do that here and directly move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6534b7-10b3-409e-b0ff-5bbe7c4cfca4",
   "metadata": {},
   "source": [
    "3. Text cleaning : Text cleaning again refers to lemmatizing the words, tokenising them and also removing any digits and special characters. And also converting all the words into lowercase for ease of access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea045c-2929-4b42-a3d4-ff4c624e91d9",
   "metadata": {},
   "source": [
    "Loading the english language NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01fa578b-2f17-4f58-87c3-9eaffb971da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17245b14-89d7-4cc3-9935-dad0431079c1",
   "metadata": {},
   "source": [
    "Printing the word, its part of speech, dependency and its lemmatised form..from the first paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cc38077-8cc3-433b-a3da-7da4e292db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "Web -- NOUN -- compound -- web\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "scraping -- NOUN -- nsubj -- scraping\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ", -- PUNCT -- punct -- ,\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "the -- DET -- det -- the\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "process -- NOUN -- appos -- process\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "of -- ADP -- prep -- of\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "extracting -- VERB -- pcomp -- extract\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "data -- NOUN -- dobj -- datum\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "from -- ADP -- prep -- from\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "websites -- NOUN -- pobj -- website\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ", -- PUNCT -- punct -- ,\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "has -- AUX -- aux -- have\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "emerged -- VERB -- ROOT -- emerge\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "as -- ADP -- prep -- as\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "a -- DET -- det -- a\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "powerful -- ADJ -- amod -- powerful\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "technique -- NOUN -- pobj -- technique\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "to -- PART -- aux -- to\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "gather -- VERB -- advcl -- gather\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "information -- NOUN -- dobj -- information\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "from -- ADP -- prep -- from\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "the -- DET -- det -- the\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "vast -- ADJ -- amod -- vast\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "expanse -- NOUN -- pobj -- expanse\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "of -- ADP -- prep -- of\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "the -- DET -- det -- the\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "internet -- NOUN -- pobj -- internet\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ". -- PUNCT -- punct -- .\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "In -- ADP -- prep -- in\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "this -- DET -- det -- this\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "tutorial -- NOUN -- pobj -- tutorial\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ", -- PUNCT -- punct -- ,\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "we -- PRON -- nsubj -- we\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "’ll -- AUX -- aux -- ’ll\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "explore -- VERB -- ROOT -- explore\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "various -- ADJ -- amod -- various\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "Python -- PROPN -- compound -- Python\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "libraries -- NOUN -- dobj -- library\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "and -- CCONJ -- cc -- and\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "modules -- NOUN -- conj -- module\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "commonly -- ADV -- advmod -- commonly\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "used -- VERB -- acl -- use\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "for -- ADP -- prep -- for\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "web -- NOUN -- compound -- web\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "scraping -- NOUN -- pobj -- scraping\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "and -- CCONJ -- cc -- and\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "delve -- VERB -- conj -- delve\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "into -- ADP -- prep -- into\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "why -- SCONJ -- advmod -- why\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "Python -- PROPN -- nsubj -- Python\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "3 -- NUM -- nummod -- 3\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "is -- AUX -- pcomp -- be\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "the -- DET -- det -- the\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "preferred -- ADJ -- amod -- preferred\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "choice -- NOUN -- attr -- choice\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "for -- ADP -- prep -- for\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "this -- DET -- det -- this\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "task -- NOUN -- pobj -- task\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ". -- PUNCT -- punct -- .\n"
     ]
    }
   ],
   "source": [
    "doc_1 = nlp(first_para)\n",
    "for token in doc_1:\n",
    "    print('Word', '--', 'Parts of Speech', '--', 'Dependency', '--', 'Lemmatized word')\n",
    "    print(token.text, '--', token.pos_, '--', token.dep_, '--', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcf18c-23b5-469c-8746-cbc6dfc9cfc0",
   "metadata": {},
   "source": [
    "Performing stemming and printing the stemmed words of each word in the first paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f50ae0-5726-4f85-bf8d-27a838c8cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95cd5f4a-3bfa-4785-9ee9-62c16b11f8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word -- Stemmed word\n",
      "Web '--' web\n",
      "Word -- Stemmed word\n",
      "scraping '--' scrape\n",
      "Word -- Stemmed word\n",
      ", '--' ,\n",
      "Word -- Stemmed word\n",
      "the '--' the\n",
      "Word -- Stemmed word\n",
      "process '--' process\n",
      "Word -- Stemmed word\n",
      "of '--' of\n",
      "Word -- Stemmed word\n",
      "extracting '--' extract\n",
      "Word -- Stemmed word\n",
      "data '--' data\n",
      "Word -- Stemmed word\n",
      "from '--' from\n",
      "Word -- Stemmed word\n",
      "websites '--' websit\n",
      "Word -- Stemmed word\n",
      ", '--' ,\n",
      "Word -- Stemmed word\n",
      "has '--' ha\n",
      "Word -- Stemmed word\n",
      "emerged '--' emerg\n",
      "Word -- Stemmed word\n",
      "as '--' as\n",
      "Word -- Stemmed word\n",
      "a '--' a\n",
      "Word -- Stemmed word\n",
      "powerful '--' power\n",
      "Word -- Stemmed word\n",
      "technique '--' techniqu\n",
      "Word -- Stemmed word\n",
      "to '--' to\n",
      "Word -- Stemmed word\n",
      "gather '--' gather\n",
      "Word -- Stemmed word\n",
      "information '--' inform\n",
      "Word -- Stemmed word\n",
      "from '--' from\n",
      "Word -- Stemmed word\n",
      "the '--' the\n",
      "Word -- Stemmed word\n",
      "vast '--' vast\n",
      "Word -- Stemmed word\n",
      "expanse '--' expans\n",
      "Word -- Stemmed word\n",
      "of '--' of\n",
      "Word -- Stemmed word\n",
      "the '--' the\n",
      "Word -- Stemmed word\n",
      "internet '--' internet\n",
      "Word -- Stemmed word\n",
      ". '--' .\n",
      "Word -- Stemmed word\n",
      "In '--' in\n",
      "Word -- Stemmed word\n",
      "this '--' thi\n",
      "Word -- Stemmed word\n",
      "tutorial '--' tutori\n",
      "Word -- Stemmed word\n",
      ", '--' ,\n",
      "Word -- Stemmed word\n",
      "we '--' we\n",
      "Word -- Stemmed word\n",
      "’ll '--' ’ll\n",
      "Word -- Stemmed word\n",
      "explore '--' explor\n",
      "Word -- Stemmed word\n",
      "various '--' variou\n",
      "Word -- Stemmed word\n",
      "Python '--' python\n",
      "Word -- Stemmed word\n",
      "libraries '--' librari\n",
      "Word -- Stemmed word\n",
      "and '--' and\n",
      "Word -- Stemmed word\n",
      "modules '--' modul\n",
      "Word -- Stemmed word\n",
      "commonly '--' commonli\n",
      "Word -- Stemmed word\n",
      "used '--' use\n",
      "Word -- Stemmed word\n",
      "for '--' for\n",
      "Word -- Stemmed word\n",
      "web '--' web\n",
      "Word -- Stemmed word\n",
      "scraping '--' scrape\n",
      "Word -- Stemmed word\n",
      "and '--' and\n",
      "Word -- Stemmed word\n",
      "delve '--' delv\n",
      "Word -- Stemmed word\n",
      "into '--' into\n",
      "Word -- Stemmed word\n",
      "why '--' whi\n",
      "Word -- Stemmed word\n",
      "Python '--' python\n",
      "Word -- Stemmed word\n",
      "3 '--' 3\n",
      "Word -- Stemmed word\n",
      "is '--' is\n",
      "Word -- Stemmed word\n",
      "the '--' the\n",
      "Word -- Stemmed word\n",
      "preferred '--' prefer\n",
      "Word -- Stemmed word\n",
      "choice '--' choic\n",
      "Word -- Stemmed word\n",
      "for '--' for\n",
      "Word -- Stemmed word\n",
      "this '--' thi\n",
      "Word -- Stemmed word\n",
      "task '--' task\n",
      "Word -- Stemmed word\n",
      ". '--' .\n"
     ]
    }
   ],
   "source": [
    "for word in doc_1:\n",
    "    print('Word', '--', 'Stemmed word')\n",
    "    print(f\"{word} -- {p_stemmer.stem(str(word))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9057d7b-3316-4f60-b78f-53a464cbeee4",
   "metadata": {},
   "source": [
    "Printing the word, its part of speech, dependency and the lemmatised form of the word...from the second paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a63b268-aaa2-4a58-9fe7-03e33618a3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "The -- DET -- det -- the\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "latest -- ADJ -- amod -- late\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "version -- NOUN -- nsubj -- version\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "of -- ADP -- prep -- of\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "Python -- PROPN -- pobj -- Python\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ", -- PUNCT -- punct -- ,\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "offers -- VERB -- ROOT -- offer\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "a -- DET -- det -- a\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "rich -- ADJ -- amod -- rich\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "set -- NOUN -- dobj -- set\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "of -- ADP -- prep -- of\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "tools -- NOUN -- pobj -- tool\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "and -- CCONJ -- cc -- and\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "libraries -- NOUN -- conj -- library\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "specifically -- ADV -- advmod -- specifically\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "designed -- VERB -- acl -- design\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "for -- ADP -- prep -- for\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "web -- NOUN -- compound -- web\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "scraping -- NOUN -- pobj -- scraping\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ", -- PUNCT -- punct -- ,\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "making -- VERB -- advcl -- make\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "it -- PRON -- nsubj -- it\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "easier -- ADJ -- ccomp -- easy\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "than -- ADP -- prep -- than\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "ever -- ADV -- pcomp -- ever\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "to -- PART -- aux -- to\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "retrieve -- VERB -- advcl -- retrieve\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "data -- NOUN -- dobj -- datum\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "from -- ADP -- prep -- from\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "the -- DET -- det -- the\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "web -- NOUN -- pobj -- web\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "efficiently -- ADV -- advmod -- efficiently\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "and -- CCONJ -- cc -- and\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      "effectively -- ADV -- conj -- effectively\n",
      "Word -- Parts of Speech -- Dependency -- Lemmatized word\n",
      ". -- PUNCT -- punct -- .\n"
     ]
    }
   ],
   "source": [
    "doc_2 = nlp(second_para)\n",
    "for token in doc_2:\n",
    "    print('Word', '--', 'Parts of Speech', '--', 'Dependency', '--', 'Lemmatized word')\n",
    "    print(token.text, '--', token.pos_, '--', token.dep_, '--', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a3a3b-4515-491d-a18f-5abc0aedb2db",
   "metadata": {},
   "source": [
    "Performing stemming and printing the stemmed words of each word in the second paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb534c4-629f-4da8-b2e3-d28936b8f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e231a8-2822-454d-aba5-edd281916212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word -- Stemmed word\n",
      "The '--' the\n",
      "Word -- Stemmed word\n",
      "latest '--' latest\n",
      "Word -- Stemmed word\n",
      "version '--' version\n",
      "Word -- Stemmed word\n",
      "of '--' of\n",
      "Word -- Stemmed word\n",
      "Python '--' python\n",
      "Word -- Stemmed word\n",
      ", '--' ,\n",
      "Word -- Stemmed word\n",
      "offers '--' offer\n",
      "Word -- Stemmed word\n",
      "a '--' a\n",
      "Word -- Stemmed word\n",
      "rich '--' rich\n",
      "Word -- Stemmed word\n",
      "set '--' set\n",
      "Word -- Stemmed word\n",
      "of '--' of\n",
      "Word -- Stemmed word\n",
      "tools '--' tool\n",
      "Word -- Stemmed word\n",
      "and '--' and\n",
      "Word -- Stemmed word\n",
      "libraries '--' librari\n",
      "Word -- Stemmed word\n",
      "specifically '--' specif\n",
      "Word -- Stemmed word\n",
      "designed '--' design\n",
      "Word -- Stemmed word\n",
      "for '--' for\n",
      "Word -- Stemmed word\n",
      "web '--' web\n",
      "Word -- Stemmed word\n",
      "scraping '--' scrape\n",
      "Word -- Stemmed word\n",
      ", '--' ,\n",
      "Word -- Stemmed word\n",
      "making '--' make\n",
      "Word -- Stemmed word\n",
      "it '--' it\n",
      "Word -- Stemmed word\n",
      "easier '--' easier\n",
      "Word -- Stemmed word\n",
      "than '--' than\n",
      "Word -- Stemmed word\n",
      "ever '--' ever\n",
      "Word -- Stemmed word\n",
      "to '--' to\n",
      "Word -- Stemmed word\n",
      "retrieve '--' retriev\n",
      "Word -- Stemmed word\n",
      "data '--' data\n",
      "Word -- Stemmed word\n",
      "from '--' from\n",
      "Word -- Stemmed word\n",
      "the '--' the\n",
      "Word -- Stemmed word\n",
      "web '--' web\n",
      "Word -- Stemmed word\n",
      "efficiently '--' effici\n",
      "Word -- Stemmed word\n",
      "and '--' and\n",
      "Word -- Stemmed word\n",
      "effectively '--' effect\n",
      "Word -- Stemmed word\n",
      ". '--' .\n"
     ]
    }
   ],
   "source": [
    "for word in doc_2:\n",
    "    print('Word', '--', 'Stemmed word')\n",
    "    print(f\"{word} -- {s_stemmer.stem(str(word))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f0be8-6c51-4a7e-a32e-0362c26d8f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
